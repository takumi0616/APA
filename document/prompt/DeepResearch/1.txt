現在以下のシステムを開発中です

```
# 改訂版 memo（整理・清書）

目的：
カメラ映像から紙フォームを検出し、フォーム種別（A/B/その他）を判定したうえで、テンプレートに対して自動補正しリアルタイム表示する。

前提：
・リアルタイム処理はCPU中心（軽量優先）

--------------------------------------------
1. 入力（リアルタイム）
--------------------------------------------
- カメラからフレームを連続取得
- 事前にレンズ歪み補正（undistort）を適用（精度・見栄え向上）


--------------------------------------------
2. 検出フェーズ（軽量・頑健）
--------------------------------------------
(2-1) 紙検出（優先）
- 輪郭検出・直線検出などで「紙らしい四角形」を推定
- 紙候補を得る

(2-2)  紙検出部分からQR / マーカー検出（A or B or etc...）
- QRコードやマーカーを高速に探索
- 検出を判定

（理由）
カメラ全画面からQRやマーカーを認識するのは非常に誤検出のリスクがある
また、用紙のフォーム別検出に関する豊富なデータや先行研究モデルもないことから、背景込みから特定の模様や特徴を探すのは困難と判断
しかし、紙の検出であれば先行研究モデルが存在している


--------------------------------------------
3. フォーム種別判定（A/B/その他：ここは追加可能なようにモジュール実装）
--------------------------------------------
- フォームA：マーカー（例：正方形マーカー）で識別
- フォームB：QRコードで識別
- その他：必要に応じた識別方法（ロゴ等）

※基本方針：
フォームの変更は絶対に不可
「識別できた特徴点（QR四隅／マーカー角点など）を、整列用の対応点として使える形で保持する」


--------------------------------------------
4. 補正画像の生成
--------------------------------------------
- 1フレームの画像、それに対して
  - 紙検出時の四角形ボックス
  - モジュールの抽出された特徴点情報
が得られるので...

(4-1) 位置合わせ（Homography）
- フレームで Homography を推定
  - もしくは「追跡」で更新し、安定なワープを得る
- テンプレ座標系へ warp（透視変換）した画像を蓄積する
- 紙のたわみや角度的なずれをここで改善

(4-2) 画像の品質改善
- ブラー（ピンぼけ/手ブレ）改善
- ノイズ除去
- 文字化け改善（2値化等）
- 影の改善（照明ムラ補正）
- 汚れ・しみ補正
etc...

完璧にきれいにする


--------------------------------------------
5. 表示
--------------------------------------------
- 補正された画像を標準形式でリアルタイム表示
- メタ情報も表示（フォームID、スコア、検出成功率、処理時間など）

↓

OCR作業へ（担当外）

```

テスト用にプログラムも作成しています

C:\Users\takumi\develop\APA\test_docaligner_camera_v2.py
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DocAligner Real-time Document Detection v2
==========================================

改善版: 手による精度低下を軽減する機能を追加

Features:
1. 時間的平滑化（複数フレームの結果を平均化）
2. モデル選択機能（軽量/高精度）
3. 安定性フィルタ（異常値除去）
4. ROI安定化

Controls:
- 'q' or ESC: 終了
- 's': 画像保存
- 'p': 透視変換プレビュー
- 'm': モデル切り替え（heatmap/point）
- '1': lcnet050 (Point, 最軽量)
- '2': lcnet100 (Heatmap, バランス)
- '3': fastvit_t8 (Heatmap, 軽量)
- '4': fastvit_sa24 (Heatmap, 最高精度)

Created: January 6, 2026
"""

import cv2
import numpy as np
import sys
import os
import warnings
from datetime import datetime
from collections import deque

# Fix encoding for Windows
sys.stdout.reconfigure(encoding='utf-8')
os.environ['PYTHONIOENCODING'] = 'utf-8'

# Suppress warnings
warnings.filterwarnings('ignore')

# Import DocAligner
from docaligner import DocAligner, ModelType
import capybara as cb


class PolygonSmoother:
    """
    時間的平滑化クラス
    複数フレームの結果を平均化して安定性を向上
    """
    def __init__(self, buffer_size=3, outlier_threshold=100):
        self.buffer = deque(maxlen=buffer_size)
        self.outlier_threshold = outlier_threshold
        self.last_valid_polygon = None
        self.no_detect_count = 0
        self.max_no_detect = 10  # この回数検出なしで前回結果をクリア
    
    def update(self, polygon, use_filter=True):
        """新しいポリゴンを追加して平滑化"""
        if polygon is None or len(polygon) < 4:
            self.no_detect_count += 1
            # 一定回数検出なしでリセット
            if self.no_detect_count > self.max_no_detect:
                self.last_valid_polygon = None
                self.buffer.clear()
            elif self.last_valid_polygon is not None:
                return self.last_valid_polygon
            return None
        
        self.no_detect_count = 0
        
        # フィルタが無効の場合は直接返す
        if not use_filter:
            self.last_valid_polygon = polygon
            return polygon
        
        # 異常値チェック（閾値を大きくして手の動きに対応）
        if self.last_valid_polygon is not None:
            diff = np.abs(polygon - self.last_valid_polygon).max()
            if diff > self.outlier_threshold:
                # 大きな変化でも受け入れる（手の位置が変わった可能性）
                self.buffer.clear()  # バッファをクリアして新しい位置に追従
        
        self.buffer.append(polygon.copy())
        self.last_valid_polygon = polygon
        
        if len(self.buffer) < 2:
            return polygon
        
        # 中央値を使用（外れ値に強い）
        stacked = np.stack(list(self.buffer))
        smoothed = np.median(stacked, axis=0)
        
        return smoothed
    
    def reset(self):
        """バッファをリセット"""
        self.buffer.clear()
        self.last_valid_polygon = None
        self.no_detect_count = 0


def set_camera_resolution(cap, width, height, fps):
    """カメラ解像度とFPSを設定"""
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
    cap.set(cv2.CAP_PROP_FPS, fps)
    
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    
    return actual_width, actual_height, actual_fps


def save_frame(frame, save_dir="docaligner_captures_v2"):
    """フレームを保存"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    full_path = os.path.join(script_dir, save_dir)
    if not os.path.exists(full_path):
        os.makedirs(full_path)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"doc_{timestamp}.jpg"
    filepath = os.path.join(full_path, filename)
    
    cv2.imwrite(filepath, frame)
    return filepath


def expand_polygon(polygon, margin=20):
    """
    ポリゴンを外側に拡大する
    
    Args:
        polygon: 4点のポリゴン
        margin: 拡大するピクセル数
    
    Returns:
        拡大されたポリゴン
    """
    if polygon is None or len(polygon) < 4:
        return polygon
    
    # 中心点を計算
    center = polygon.mean(axis=0)
    
    # 各点を中心から外側に移動
    expanded = []
    for pt in polygon:
        direction = pt - center
        # 方向を正規化して拡大
        length = np.linalg.norm(direction)
        if length > 0:
            unit_direction = direction / length
            new_pt = pt + unit_direction * margin
        else:
            new_pt = pt
        expanded.append(new_pt)
    
    return np.array(expanded)


def draw_polygon(frame, polygon, color=(0, 255, 0), thickness=3, expand_margin=0):
    """ポリゴンを描画"""
    if polygon is None or len(polygon) < 4:
        return frame
    
    # マージンが指定されている場合は拡大
    if expand_margin > 0:
        polygon = expand_polygon(polygon, expand_margin)
    
    result = frame.copy()
    pts = polygon.astype(np.int32)
    
    # 半透明の塗りつぶし
    overlay = frame.copy()
    cv2.fillPoly(overlay, [pts], color)
    cv2.addWeighted(overlay, 0.2, result, 0.8, 0, result)
    
    # アウトライン
    cv2.polylines(result, [pts], True, color, thickness)
    
    # コーナーポイント
    for i, pt in enumerate(pts):
        cv2.circle(result, tuple(pt), 8, (0, 0, 255), -1)
        cv2.circle(result, tuple(pt), 10, (255, 255, 255), 2)
        labels = ['TL', 'TR', 'BR', 'BL']
        cv2.putText(result, labels[i], (pt[0] + 15, pt[1] + 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    return result


def draw_info_panel(frame, fps, doc_detected, model_name, smoothing_enabled):
    """情報パネルを描画"""
    result = frame.copy()
    
    # 背景パネル
    cv2.rectangle(result, (5, 5), (400, 130), (0, 0, 0), -1)
    cv2.rectangle(result, (5, 5), (400, 130), (0, 255, 0), 1)
    
    # FPS
    cv2.putText(result, f"FPS: {fps:.1f}", (10, 25),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
    
    # 検出状態
    status_color = (0, 255, 0) if doc_detected else (0, 0, 255)
    status_text = "Document: DETECTED" if doc_detected else "Document: NOT FOUND"
    cv2.putText(result, status_text, (10, 50),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 1)
    
    # モデル情報
    cv2.putText(result, f"Model: {model_name}", (10, 75),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    # 平滑化状態
    smooth_text = "Smoothing: ON" if smoothing_enabled else "Smoothing: OFF"
    smooth_color = (0, 255, 0) if smoothing_enabled else (100, 100, 100)
    cv2.putText(result, smooth_text, (10, 95),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, smooth_color, 1)
    
    # キー操作ガイド
    cv2.putText(result, "Keys: q=Exit s=Save m=Model 1-4=Select", (10, 115),
                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
    
    return result


def get_perspective_transform(frame, polygon, output_size=(800, 600)):
    """透視変換を適用"""
    if polygon is None or len(polygon) < 4:
        return None
    
    dst = np.array([
        [0, 0],
        [output_size[0] - 1, 0],
        [output_size[0] - 1, output_size[1] - 1],
        [0, output_size[1] - 1]
    ], dtype="float32")
    
    M = cv2.getPerspectiveTransform(polygon.astype(np.float32), dst)
    warped = cv2.warpPerspective(frame, M, output_size)
    
    return warped


# 利用可能なモデル設定
MODEL_CONFIGS = {
    '1': {'type': ModelType.point, 'cfg': 'lcnet050', 'name': 'lcnet050 (Point, 最軽量)'},
    '2': {'type': ModelType.heatmap, 'cfg': 'lcnet100', 'name': 'lcnet100 (Heatmap, バランス)'},
    '3': {'type': ModelType.heatmap, 'cfg': 'fastvit_t8', 'name': 'fastvit_t8 (Heatmap, 軽量)'},
    '4': {'type': ModelType.heatmap, 'cfg': 'fastvit_sa24', 'name': 'fastvit_sa24 (Heatmap, 最高精度)'},
}


def load_model(key='4'):
    """モデルをロード"""
    config = MODEL_CONFIGS.get(key, MODEL_CONFIGS['4'])
    print(f"Loading model: {config['name']}...")
    model = DocAligner(
        model_type=config['type'],
        model_cfg=config['cfg']
    )
    print(f"[OK] Model loaded: {config['name']}")
    return model, config['name']


def main():
    """メイン関数: リアルタイム書類検出（改善版）"""
    print()
    print("=" * 60)
    print("DocAligner Real-time Document Detection v2")
    print("改善版: 時間的平滑化 + モデル選択")
    print("=" * 60)
    print()
    print(f"OpenCV Version: {cv2.__version__}")
    print()
    
    # 利用可能なモデル表示
    print("利用可能なモデル:")
    for key, config in MODEL_CONFIGS.items():
        print(f"  [{key}] {config['name']}")
    print()
    
    # デフォルトモデルをロード（最高精度）
    current_model_key = '4'
    model, model_name = load_model(current_model_key)
    print()
    
    # カメラを開く
    print("Opening camera...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("Error: Could not open camera")
        return 1
    
    width, height, fps = set_camera_resolution(cap, 1280, 720, 30)
    print(f"[OK] Camera opened: {width}x{height} @ {fps:.1f}fps")
    print()
    
    print("=" * 60)
    print("操作方法:")
    print("  'q' or ESC: 終了")
    print("  's': 画像保存")
    print("  'p': 透視変換プレビュー")
    print("  't': 平滑化のON/OFF切り替え")
    print("  '1'-'4': モデル切り替え")
    print("  '+'/'-': ボックスマージン調整")
    print("=" * 60)
    print()
    
    # 状態変数
    show_perspective = False
    smoothing_enabled = True
    smoother = PolygonSmoother(buffer_size=5, outlier_threshold=50)
    box_margin = 30  # デフォルトマージン（ピクセル）
    
    prev_time = cv2.getTickCount()
    fps_display = 0.0
    frame_count = 0
    
    main_window = "DocAligner v2 - Document Detection"
    perspective_window = "Perspective Correction"
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Warning: Failed to get frame")
                break
            
            # FPS計算
            current_time = cv2.getTickCount()
            time_diff = (current_time - prev_time) / cv2.getTickFrequency()
            if time_diff >= 1.0:
                fps_display = frame_count / time_diff
                frame_count = 0
                prev_time = current_time
            frame_count += 1
            
            # パディング追加
            padded_frame = cb.pad(frame, 100)
            
            # DocAlignerで検出
            polygon = model(
                img=padded_frame,
                do_center_crop=False
            )
            
            # パディング分を引く
            if polygon is not None:
                polygon = polygon - 100
            
            # 平滑化適用
            if smoothing_enabled:
                polygon = smoother.update(polygon)
            
            doc_detected = polygon is not None and len(polygon) >= 4
            
            # 結果を描画（マージン付き）
            result = frame.copy()
            if doc_detected:
                result = draw_polygon(result, polygon, expand_margin=box_margin)
            
            result = draw_info_panel(result, fps_display, doc_detected, 
                                     model_name, smoothing_enabled)
            
            # マージン表示
            cv2.putText(result, f"Margin: {box_margin}px (+/-)", (width - 200, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            cv2.imshow(main_window, result)
            
            # 透視変換ウィンドウ
            if show_perspective and doc_detected:
                warped = get_perspective_transform(frame, polygon)
                if warped is not None:
                    cv2.imshow(perspective_window, warped)
            else:
                try:
                    cv2.destroyWindow(perspective_window)
                except:
                    pass
            
            # キー処理
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q') or key == 27:
                print("\n終了します...")
                break
            elif key == ord('s'):
                filepath = save_frame(result)
                print(f"保存しました: {filepath}")
                if doc_detected:
                    warped = get_perspective_transform(frame, polygon)
                    if warped is not None:
                        warped_path = filepath.replace('.jpg', '_corrected.jpg')
                        cv2.imwrite(warped_path, warped)
                        print(f"補正画像を保存: {warped_path}")
            elif key == ord('p'):
                show_perspective = not show_perspective
                print(f"透視変換プレビュー: {'ON' if show_perspective else 'OFF'}")
            elif key == ord('t'):
                smoothing_enabled = not smoothing_enabled
                if not smoothing_enabled:
                    smoother.reset()
                print(f"平滑化: {'ON' if smoothing_enabled else 'OFF'}")
            elif key == ord('+') or key == ord('='):
                box_margin = min(100, box_margin + 10)
                print(f"マージン: {box_margin}px")
            elif key == ord('-') or key == ord('_'):
                box_margin = max(0, box_margin - 10)
                print(f"マージン: {box_margin}px")
            elif chr(key) in MODEL_CONFIGS:
                new_key = chr(key)
                if new_key != current_model_key:
                    current_model_key = new_key
                    model, model_name = load_model(current_model_key)
                    smoother.reset()
    
    except KeyboardInterrupt:
        print("\n\nCtrl+Cで終了...")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
    
    print()
    print("=" * 60)
    print("DocAligner Document Detection v2 Complete")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

```


C:\Users\takumi\develop\APA\test_capture_formA.py
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
フォームA画像の3点マーク検出テストプログラム（改善版）

目的:
- image/Aディレクトリの画像から3点マーク（黒い四角形）を検出
- 上左、上右、下左の3箇所にあるマーカーを検出
- 検出部分にボックスを描画

改善点:
- 複数の二値化方法を試行（適応的閾値、Otsu法など）
- 閾値を緩和して色違いの画像にも対応
- 描画線を太くする
"""

import cv2
import numpy as np
import os
import sys
import time

# Windows環境でUTF-8を強制
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.stdout:
    sys.stdout.reconfigure(encoding='utf-8')


def detect_filled_square_markers(image, target_corners=['top_left', 'top_right', 'bottom_left']):
    """
    塗りつぶしの四角マーカー（3点マーク）を検出する（改善版）
    
    Args:
        image: OpenCV画像（BGR形式）
        target_corners: 検出対象のコーナー
    
    Returns:
        list: 検出された四角マーカーの情報リスト
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    image_height, image_width = image.shape[:2]
    
    markers = []
    
    # 複数の二値化方法を試行
    binary_images = []
    
    # 方法1: 固定閾値（低め - 薄いマーカー用）
    _, binary1 = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
    binary_images.append(('固定閾値50', binary1))
    
    # 方法2: 固定閾値（中程度）
    _, binary2 = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV)
    binary_images.append(('固定閾値80', binary2))
    
    # 方法3: 固定閾値（高め）
    _, binary3 = cv2.threshold(gray, 120, 255, cv2.THRESH_BINARY_INV)
    binary_images.append(('固定閾値120', binary3))
    
    # 方法4: Otsu法
    _, binary4 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    binary_images.append(('Otsu', binary4))
    
    # 方法5: 適応的閾値（ブロックサイズ小）
    binary5 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                     cv2.THRESH_BINARY_INV, 21, 8)
    binary_images.append(('適応的21', binary5))
    
    # 方法6: 適応的閾値（ブロックサイズ大）
    binary6 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                     cv2.THRESH_BINARY_INV, 51, 10)
    binary_images.append(('適応的51', binary6))
    
    # 画像の4隅の領域を定義（画像端から15%の範囲）
    corner_margin_x = int(image_width * 0.15)
    corner_margin_y = int(image_height * 0.15)
    
    corners = {
        'top_left': (0, 0, corner_margin_x, corner_margin_y),
        'top_right': (image_width - corner_margin_x, 0, image_width, corner_margin_y),
        'bottom_left': (0, image_height - corner_margin_y, corner_margin_x, image_height),
        'bottom_right': (image_width - corner_margin_x, image_height - corner_margin_y, image_width, image_height)
    }
    
    # マーカーサイズの範囲（画像サイズに基づく）
    min_size = min(image_width, image_height) * 0.005
    max_size = min(image_width, image_height) * 0.08
    min_area = min_size ** 2
    max_area = max_size ** 2
    
    found_corners = {}  # コーナーごとに最良のマーカーを保持
    
    for method_name, binary in binary_images:
        # モルフォロジー演算
        kernel = np.ones((3, 3), np.uint8)
        binary_clean = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        binary_clean = cv2.morphologyEx(binary_clean, cv2.MORPH_OPEN, kernel)
        
        contours, _ = cv2.findContours(binary_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            area = w * h
            contour_area = cv2.contourArea(contour)
            
            if min_area < contour_area < max_area:
                aspect_ratio = float(w) / h if h > 0 else 0
                # アスペクト比を緩和（0.4〜2.5）
                if 0.4 < aspect_ratio < 2.5:
                    cx = x + w // 2
                    cy = y + h // 2
                    
                    corner_name = None
                    for name, (x1, y1, x2, y2) in corners.items():
                        if x1 <= cx <= x2 and y1 <= cy <= y2:
                            corner_name = name
                            break
                    
                    if corner_name in target_corners:
                        fill_ratio = contour_area / area if area > 0 else 0
                        
                        # 塗りつぶし率を緩和（0.4以上）
                        if fill_ratio > 0.4:
                            mask = np.zeros(gray.shape, dtype=np.uint8)
                            cv2.drawContours(mask, [contour], 0, 255, -1)
                            mean_val = cv2.mean(gray, mask=mask)[0]
                            
                            # 輝度閾値を緩和（180未満）
                            if mean_val < 180:
                                epsilon = 0.05 * cv2.arcLength(contour, True)
                                approx = cv2.approxPolyDP(contour, epsilon, True)
                                
                                # スコア計算
                                aspect_score = 1.0 - abs(aspect_ratio - 1.0) * 0.5
                                fill_score = fill_ratio
                                intensity_score = (180 - mean_val) / 180.0
                                score = aspect_score * 0.25 + fill_score * 0.35 + intensity_score * 0.4
                                
                                marker_info = {
                                    'contour': contour,
                                    'approx': approx,
                                    'area': contour_area,
                                    'center': (cx, cy),
                                    'bbox': (x, y, w, h),
                                    'corner': corner_name,
                                    'aspect_ratio': aspect_ratio,
                                    'fill_ratio': fill_ratio,
                                    'mean_intensity': mean_val,
                                    'vertices': len(approx),
                                    'score': score,
                                    'method': method_name
                                }
                                
                                # 既存のマーカーよりスコアが高ければ更新
                                if corner_name not in found_corners or score > found_corners[corner_name]['score']:
                                    found_corners[corner_name] = marker_info
    
    # 見つかったマーカーをリストに変換
    markers = list(found_corners.values())
    return markers


def draw_detections(image, markers):
    """
    検出結果を画像に描画する（線を太くする）
    """
    result = image.copy()
    
    height, width = image.shape[:2]
    scale = min(width, height) / 1000.0
    font_scale = max(0.8, scale * 1.0)
    # 線の太さを大幅に増加
    thickness = max(8, int(scale * 10))
    font_thickness = max(3, int(scale * 4))
    
    # マーカーの描画（赤色、太い線）
    for i, marker in enumerate(markers):
        x, y, w, h = marker['bbox']
        cv2.rectangle(result, (x, y), (x + w, y + h), (0, 0, 255), thickness)
        
        corner = marker['corner'] if marker['corner'] else 'unknown'
        label = f"M{i+1}:{corner}"
        label_y = max(y - 20, 50)
        cv2.putText(result, label, (x, label_y),
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), font_thickness)
    
    return result


def process_image(image_path, output_dir=None):
    """
    画像を処理してマーカーを検出する
    """
    start_time = time.time()
    
    image = cv2.imread(image_path)
    if image is None:
        print(f"  [エラー] 画像を読み込めませんでした: {image_path}")
        return None
    
    height, width = image.shape[:2]
    
    # 3点マーカー検出（上左、上右、下左）
    expected_corners = ['top_left', 'top_right', 'bottom_left']
    markers = detect_filled_square_markers(image, expected_corners)
    
    process_time = time.time() - start_time
    
    result_image = draw_detections(image, markers)
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.basename(image_path)
        output_path = os.path.join(output_dir, f"detected_{filename}")
        cv2.imwrite(output_path, result_image)
    
    return {
        'image_path': image_path,
        'size': (width, height),
        'markers': markers,
        'process_time': process_time,
        'result_image': result_image
    }


def main():
    """メイン関数"""
    print("="*70)
    print("フォームA画像 - 3点マーク検出テスト（改善版）")
    print("="*70)
    
    # image/Aディレクトリのみをテスト
    base_dir = os.path.dirname(__file__)
    image_dir = os.path.join(base_dir, "image", "A")
    output_dir = os.path.join(base_dir, "output", "formA_test")
    
    # 画像ファイルを取得
    all_files = []
    for i in range(1, 7):
        image_path = os.path.join(image_dir, f"{i}.jpg")
        if os.path.exists(image_path):
            all_files.append(image_path)
    
    if not all_files:
        print(f"\n[エラー] 画像が見つかりません: {image_dir}")
        return
    
    print(f"\n検出対象: image/A ディレクトリ（3点マーク検出）")
    print(f"画像数: {len(all_files)}枚")
    print("-"*70)
    
    results = []
    total_start = time.time()
    
    for image_path in all_files:
        filename = os.path.basename(image_path)
        
        result = process_image(image_path, output_dir)
        if result:
            results.append(result)
            
            marker_count = len(result['markers'])
            status = "✓" if marker_count == 3 else f"✗({marker_count}/3)"
            markers_str = ", ".join([m['corner'] for m in result['markers']])
            
            print(f"  {filename}: {result['size'][0]}x{result['size'][1]}, "
                  f"マーカー: {status} [{markers_str}], "
                  f"時間: {result['process_time']*1000:.0f}ms")
    
    total_time = time.time() - total_start
    
    # 結果サマリー
    print("\n" + "="*70)
    print("検出結果サマリー")
    print("="*70)
    
    success_count = sum(1 for r in results if len(r['markers']) == 3)
    avg_time = sum(r['process_time'] for r in results) / len(results) if results else 0
    
    print(f"\n成功: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    print(f"平均処理時間: {avg_time*1000:.0f}ms/枚")
    print(f"総処理時間: {total_time:.1f}秒")
    print(f"出力先: {output_dir}")
    
    # 詳細表示
    print("\n【詳細】")
    for result in results:
        filename = os.path.basename(result['image_path'])
        marker_count = len(result['markers'])
        status = "✓ 成功" if marker_count == 3 else f"✗ 失敗({marker_count}/3)"
        markers_str = ", ".join([f"{m['corner']}({m['method']})" for m in result['markers']])
        print(f"  {filename}: {status}")
        if result['markers']:
            print(f"    検出: [{markers_str}]")
    
    print("\n" + "="*70)
    print("テスト完了")
    print("="*70)


if __name__ == "__main__":
    main()

```


C:\Users\takumi\develop\APA\test_capture_formB.py
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
フォームB画像のQRコード検出テストプログラム（改善版）

目的:
- image/Bディレクトリの画像からQRコードを検出
- 検出部分にボックスを描画

改善点:
- OpenCV QRCodeDetectorのマルチスケール検出
- 様々な解像度で検出を試みる
- 描画線を太くする
"""

import cv2
import numpy as np
import os
import sys
import time

# Windows環境でUTF-8を強制
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.stdout:
    sys.stdout.reconfigure(encoding='utf-8')


def detect_qr_codes(image):
    """
    QRコードを検出する（OpenCVマルチスケール）
    
    Args:
        image: OpenCV画像（BGR形式）
    
    Returns:
        list: 検出されたQRコードの情報リスト
    """
    qr_results = []
    qr_detector = cv2.QRCodeDetector()
    height, width = image.shape[:2]
    
    # 複数のスケールで検出を試みる
    scales = [0.5, 0.25, 1.0, 0.125, 0.75]
    
    for scale in scales:
        if scale == 1.0:
            test_image = image
        else:
            new_width = int(width * scale)
            new_height = int(height * scale)
            if new_width < 100 or new_height < 100:
                continue
            test_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
        
        try:
            data, points, straight_qrcode = qr_detector.detectAndDecode(test_image)
            if data and points is not None:
                pts = points[0]
                if scale != 1.0:
                    pts = pts / scale
                pts_int = pts.astype(np.int32)
                qr_results.append({
                    'data': data,
                    'points': pts_int,
                    'rect': cv2.boundingRect(pts_int),
                    'scale': scale
                })
                break
        except:
            pass
    
    return qr_results


def draw_detections(image, qr_codes):
    """
    検出結果を画像に描画する（線を太くする）
    """
    result = image.copy()
    
    height, width = image.shape[:2]
    scale = min(width, height) / 1000.0
    font_scale = max(1.0, scale * 1.2)
    # 線の太さを大幅に増加
    thickness = max(10, int(scale * 15))
    font_thickness = max(4, int(scale * 5))
    
    # QRコードの描画（緑色、太い線）
    for qr in qr_codes:
        pts = qr['points'].reshape((-1, 1, 2))
        cv2.polylines(result, [pts], True, (0, 255, 0), thickness)
        
        # バウンディングボックスも描画
        x, y, w, h = qr['rect']
        cv2.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), thickness)
        
        # ラベル
        data_display = qr['data'][:25] + '...' if len(qr['data']) > 25 else qr['data']
        label_y = max(y - 30, 80)
        cv2.putText(result, f"QR: {data_display}", (x, label_y),
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), font_thickness)
    
    return result


def process_image(image_path, output_dir=None):
    """
    画像を処理してQRコードを検出する
    """
    start_time = time.time()
    
    image = cv2.imread(image_path)
    if image is None:
        print(f"  [エラー] 画像を読み込めませんでした: {image_path}")
        return None
    
    height, width = image.shape[:2]
    
    # QRコード検出
    qr_codes = detect_qr_codes(image)
    
    process_time = time.time() - start_time
    
    result_image = draw_detections(image, qr_codes)
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.basename(image_path)
        output_path = os.path.join(output_dir, f"detected_{filename}")
        cv2.imwrite(output_path, result_image)
    
    return {
        'image_path': image_path,
        'size': (width, height),
        'qr_codes': qr_codes,
        'process_time': process_time,
        'result_image': result_image
    }


def main():
    """メイン関数"""
    print("="*70)
    print("フォームB画像 - QRコード検出テスト（改善版）")
    print("="*70)
    print("QR検出: OpenCV QRCodeDetector（マルチスケール）")
    
    # image/Bディレクトリのみをテスト
    base_dir = os.path.dirname(__file__)
    image_dir = os.path.join(base_dir, "image", "B")
    output_dir = os.path.join(base_dir, "output", "formB_test")
    
    # 画像ファイルを取得
    all_files = []
    for i in range(1, 7):
        image_path = os.path.join(image_dir, f"{i}.jpg")
        if os.path.exists(image_path):
            all_files.append(image_path)
    
    if not all_files:
        print(f"\n[エラー] 画像が見つかりません: {image_dir}")
        return
    
    print(f"\n検出対象: image/B ディレクトリ（QRコード検出）")
    print(f"画像数: {len(all_files)}枚")
    print("-"*70)
    
    results = []
    total_start = time.time()
    
    for image_path in all_files:
        filename = os.path.basename(image_path)
        
        result = process_image(image_path, output_dir)
        if result:
            results.append(result)
            
            qr_count = len(result['qr_codes'])
            qr_status = "✓" if qr_count >= 1 else "✗"
            qr_data = result['qr_codes'][0]['data'] if result['qr_codes'] else '-'
            
            print(f"  {filename}: {result['size'][0]}x{result['size'][1]}, "
                  f"QR: {qr_status} ({qr_data}), "
                  f"時間: {result['process_time']*1000:.0f}ms")
    
    total_time = time.time() - total_start
    
    # 結果サマリー
    print("\n" + "="*70)
    print("検出結果サマリー")
    print("="*70)
    
    qr_success = sum(1 for r in results if len(r['qr_codes']) >= 1)
    avg_time = sum(r['process_time'] for r in results) / len(results) if results else 0
    
    print(f"\nQR検出成功: {qr_success}/{len(results)} ({qr_success/len(results)*100:.1f}%)")
    print(f"平均処理時間: {avg_time*1000:.0f}ms/枚")
    print(f"総処理時間: {total_time:.1f}秒")
    print(f"出力先: {output_dir}")
    
    # 詳細表示
    print("\n【詳細】")
    for result in results:
        filename = os.path.basename(result['image_path'])
        qr_count = len(result['qr_codes'])
        
        if qr_count >= 1:
            status = "✓ 成功"
        else:
            status = "✗ 失敗"
        
        qr_data = result['qr_codes'][0]['data'] if result['qr_codes'] else 'なし'
        
        print(f"  {filename}: {status}")
        print(f"    QR: {qr_data}")
    
    print("\n" + "="*70)
    print("テスト完了")
    print("="*70)


if __name__ == "__main__":
    main()

```

今後、myplanの補正画像の生成のシステムづくりのフェーズになっていきます
どのような技術、戦略、研究、ライブラリを使えばいいでしょうか？
調査して日本語で分かりやすく考察してください